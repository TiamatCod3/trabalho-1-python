{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# Configurando a saída padrão para UTF-8\n",
    "# sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "# O trabalho consiste na extraçaõ de notícias do site lance\n",
    "# O site tem uma estrutura interessante com elementos claros para se obter os links\n",
    "# A estratégia é buscar os links das matérias com suas slugs na página principal e depois buscar os links das matérias nos caminhos dos clubes a serem explorados\n",
    "# Ao extrair os links, novamente é feita uma iteração para buscar os conteúdos dos links pelas tags html com a seguinte estrutura: Título Principal(H1), Título Secundário(H2), conteúdo (p) e data da publicação (time)\n",
    "# Por quetões de espaço e volume de dados, foram extraídas 5 notícias de cada time e da página principal podendo variar de acordo com a definição variável\n",
    "\n",
    "#Especificando a url do site\n",
    "url_principal = \"https://www.lance.com.br/\"\n",
    "\n",
    "#Criando a array de times no formato de slugs para iteração\n",
    "times = [\n",
    "   \"atletico-mineiro\", \"atletico-paranaense\",\"bahia\",\"botafogo\", \"bragantino\",  \"corinthians\", \"criciuma\", \"cruzeiro\", \"cuiaba\",\"flamengo\", \"fluminense\", \"fortaleza\", \"gremio\", \"internacional\", \"palmeiras\", \"santos\", \"sao-paulo\", \"vasco\", \"vitoria\"\n",
    "]\n",
    "\n",
    "#Definindo número máximo de notícias por página\n",
    "maximo_noticias = 10\n",
    "\n",
    "#Criação da varíavel onde os links serão salvos\n",
    "links_gerais = []\n",
    "\n",
    "#Criação da função para obtenção dos links das páginas de acordo com a url\n",
    "def obter_links(url, n_noticias=None):\n",
    "    # Criando array vazia para coleta dos links\n",
    "    links = []\n",
    "\n",
    "    # Criando o contador de notícias\n",
    "    contador = 0\n",
    "    \n",
    "    #Obtendo os links da página principal usando o requests\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Verificando se a requisição foi bem-sucedida\n",
    "    if response.status_code == 200:\n",
    "        # Parseando o conteúdo HTML da página\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Encontrando todas as tags 'a' com atributo href\n",
    "        all_links = soup.find_all('a', href=True)\n",
    "\n",
    "        # Filtrando links que terminam com .html e não contêm \"apostas\"\n",
    "        # Havia links de propaganda direcionando para sites de aposta que foram retirados do resultado\n",
    "        html_links = [link['href'] for link in all_links if link['href'].endswith('.html') and 'apostas' not in link['href']]\n",
    "\n",
    "        # Retornando os links\n",
    "        for link in html_links:\n",
    "            # Fazendo correções de link relativo e retirando excessos de barras que foram observadas\n",
    "            if link.startswith('/'):\n",
    "                full_link = url.strip(\"/\") + link\n",
    "            else:\n",
    "                full_link = link\n",
    "            links.append(full_link)\n",
    "\n",
    "            # Verificando se foi passado o número de notícias a extrair e retorna se o valor for atingido\n",
    "            if n_noticias:\n",
    "                 contador += 1\n",
    "                 if contador >= n_noticias:\n",
    "                      return links\n",
    "\n",
    "    else:\n",
    "        print('Falha ao acessar o site')\n",
    "    \n",
    "    # Retornando a lista de links\n",
    "    return links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair data e hora\n",
    "def extrair_data_hora(data_str):\n",
    "    match = re.search(r'Publicada em (\\d{2}/\\d{2}/\\d{4} - \\d{2}:\\d{2})', data_str)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "# Função para extrair local\n",
    "def extrair_local(data_str):\n",
    "    match = re.search(r'• Publicada em \\d{2}/\\d{2}/\\d{4} - \\d{2}:\\d{2} • (.+)', data_str)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validar_estrutura(dicionario, chaves_esperadas):\n",
    "    \n",
    "    if dicionario is None:\n",
    "        return False\n",
    "    # Checando se todas as chaves esperadas estão presentes\n",
    "    if not chaves_esperadas.issubset(dicionario.keys()):\n",
    "        return False\n",
    "    \n",
    "    # Checando se os valores não são nulos\n",
    "    for chave in chaves_esperadas:\n",
    "        if dicionario[chave] is None or (isinstance(dicionario[chave], list) and not dicionario[chave]):\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_conteudo(url):\n",
    "    # Criando o dicionário vazio para coletar o conteúdo \n",
    "    conteudo = {}\n",
    "\n",
    "    # Obtendo as páginas de conteúdo das notícias\n",
    "    article_response = requests.get(url)\n",
    "\n",
    "    # Verificando a resposta\n",
    "    if article_response.status_code == 200:\n",
    "                \n",
    "                # Parseanso o conteúdo HTML da página da notícia\n",
    "                article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "                \n",
    "                # Extraindo o conteúdo da tag <article>\n",
    "                article = article_soup.find('article')\n",
    "\n",
    "                # Prevenindo extrações de páginas que não tenham a tag <article>\n",
    "                if not article:\n",
    "                     return \n",
    "                \n",
    "                if not article.find('h1'):\n",
    "                     return {}\n",
    "                # Buscando dentro do artigo o título principal e extraindo o texto pelo get_text\n",
    "                conteudo[\"Título 1\"] = article.find('h1').get_text()\n",
    "\n",
    "                if not article.find('h2'):\n",
    "                     return {}\n",
    "\n",
    "                # Buscando dentro do artigo o título principal e extraindo o texto pelo get_text\n",
    "                conteudo[\"Título 2\"] = article.find('h2').get_text()\n",
    "\n",
    "                if not article.find('time'):\n",
    "                     return {}\n",
    "\n",
    "                # Recuperando os dados de data e local da publicação na tag <time>\n",
    "                conteudo[\"Data\"] = extrair_data_hora(article.find('time').get_text())\n",
    "                conteudo[\"Local\"] = extrair_local(article.find('time').get_text())\n",
    "\n",
    "                \n",
    "\n",
    "                # Encontrando as tags de parágrafos\n",
    "                paragrafos = article.find_all('p', class_=\"w-full\")\n",
    "\n",
    "                # Recuperando a array de parágrafos obtidos\n",
    "                paragrafos = [p for p in paragrafos if not p.find('a')]\n",
    "\n",
    "                #Criando uma string vazia para inserção do conteúdo\n",
    "                conteudo[\"Conteúdo\"] = \"\"\n",
    "                for paragrafo in paragrafos:\n",
    "                \n",
    "                     # Concatenando as strings inserindo a quebra de linha no final como \\n\n",
    "                     conteudo[\"Conteúdo\"] += paragrafo.get_text() + \"\\n\"\n",
    "                # conteudo[\"Conteúdo\"] = [conteudo[\"Conteúdo\"]]\n",
    "    return conteudo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo links da página principal:\n",
    "# Chamando a função par obtenção dos links com 5 links\n",
    "links = obter_links(url_principal, maximo_noticias)\n",
    "# Concatenando os novos links recuperados na array de todos os links\n",
    "links_gerais += links\n",
    "\n",
    "# Imprimindo os tamanhos da array para verificação\n",
    "print(len(links_gerais))\n",
    "\n",
    "# Obtendo os links nas paginas dos times\n",
    "# Iterando pelo tamanho da array de times\n",
    "for i in range(len(times)):\n",
    "    # Obtendo os links por time ao concatenar a string da url principal com a string da slug do time e 5 links\n",
    "    links = obter_links(url_principal + times[i], maximo_noticias)\n",
    "\n",
    "    # Adicionando os links à lista geral de links\n",
    "    links_gerais += links\n",
    "\n",
    "    # Imprimindo os tamanhos da array para verificação\n",
    "    print(len(links_gerais))\n",
    "\n",
    "# Convertendo a lista de links gerais em um conjunto para remover as duplicatas\n",
    "links_gerais = set(links_gerais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo as chaves esperadas\n",
    "colunas_esperadas  = {\"Título 1\", \"Título 2\", \"Data\", \"Local\", \"Conteúdo\"}\n",
    "# noticias = pd.DataFrame(columns=colunas)\n",
    "# print(noticias)\n",
    "noticias = []\n",
    "for link in links_gerais:\n",
    "     conteudo = obter_conteudo(link)\n",
    "     if validar_estrutura(conteudo, colunas_esperadas):\n",
    "          noticias.append(conteudo)\n",
    "    #  print(conteudo)\n",
    "    #  conteudo = pd.DataFrame(conteudo)\n",
    "    #  noticias = pd.concat([noticias, conteudo], ignore_index=True)\n",
    "\n",
    "noticias = pd.DataFrame(noticias)\n",
    "\n",
    "# noticias.to_csv(arquivo,encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiago.matos\\AppData\\Local\\Temp\\ipykernel_8164\\2465934437.py:1: UserWarning: Parsing dates in %d/%m/%Y - %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  noticias[\"Data\"] = pd.to_datetime(noticias[\"Data\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Título 1</th>\n",
       "      <th>Título 2</th>\n",
       "      <th>Data</th>\n",
       "      <th>Local</th>\n",
       "      <th>Conteúdo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atlético-MG toma medida drástica após expulsão...</td>\n",
       "      <td>Jogo contra o Palmeiras foi marcado pela polêm...</td>\n",
       "      <td>2024-06-18 20:27:00</td>\n",
       "      <td>Belo Horizonte (MG)</td>\n",
       "      <td>- Quando John Textor fez as denúncias que todo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dono da SAF do Atlético-MG detona expulsão de ...</td>\n",
       "      <td>Empresário pediu união entre as equipes contra...</td>\n",
       "      <td>2024-06-18 15:11:00</td>\n",
       "      <td>Belo Horizonte (MG)</td>\n",
       "      <td>- O Atlético tomará todas as medidas cabíveis,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Danilo, da Seleção Brasileira, prioriza Flamen...</td>\n",
       "      <td>Lateral-direito tem contrato com a Juventus at...</td>\n",
       "      <td>2024-06-19 11:09:00</td>\n",
       "      <td>Rio de Janeiro (RJ)</td>\n",
       "      <td>– A minha família é toda flamenguista, é uma l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Números de Maurício: veja a fase atual do refo...</td>\n",
       "      <td>Meio-campista joga bastante pelo Internacional...</td>\n",
       "      <td>2024-06-19 16:27:00</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>Natural de São Paulo, Maurício iniciou sua car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carille fala sobre pressão no Santos, avalia e...</td>\n",
       "      <td>Após quatro derrotas consecutivas na Série B, ...</td>\n",
       "      <td>2024-06-19 22:38:00</td>\n",
       "      <td>São Paulo (SP)</td>\n",
       "      <td>– Me sinto confortável aqui, mas sei que se eu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Cruzeiro quer Gabigol, do Flamengo, após desis...</td>\n",
       "      <td>Atacante rubro-negro seria alvo da Raposa por ...</td>\n",
       "      <td>2024-06-18 10:06:00</td>\n",
       "      <td>Rio de Janeiro (RJ)</td>\n",
       "      <td>Sócio-majoritário da Sociedade Anônima do Fute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Lutando até o fim, Sérvia arranca empate da Es...</td>\n",
       "      <td>Seleção eslovena vencia até os 95 minutos e sé...</td>\n",
       "      <td>2024-06-20 12:07:00</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>O goleiro da Eslovênia, Oblak teve um primeiro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Veja o time ideal da 6ª rodada da Copa Sul-Ame...</td>\n",
       "      <td>Athletico conta com dois representantes na sel...</td>\n",
       "      <td>2024-06-01 07:00:00</td>\n",
       "      <td>São Paulo (SP)</td>\n",
       "      <td>Apesar dos triunfos, as equipes não contam com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Grêmio perde mais uma e iguala feito negativo ...</td>\n",
       "      <td>Imortal foi superado pelo Fortaleza, na noite ...</td>\n",
       "      <td>2024-06-20 14:16:00</td>\n",
       "      <td>Rio de Janeiro (RJ)</td>\n",
       "      <td>Após mais uma derrota, desta vez para o Fortal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Bahia x Fortaleza: onde assistir ao vivo, horá...</td>\n",
       "      <td>Confira todos os detalhes do duelo válido pela...</td>\n",
       "      <td>2024-06-13 07:46:00</td>\n",
       "      <td>Salvador (BA)</td>\n",
       "      <td>Confira todas as informações que você precisa ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Título 1  \\\n",
       "0    Atlético-MG toma medida drástica após expulsão...   \n",
       "1    Dono da SAF do Atlético-MG detona expulsão de ...   \n",
       "2    Danilo, da Seleção Brasileira, prioriza Flamen...   \n",
       "3    Números de Maurício: veja a fase atual do refo...   \n",
       "4    Carille fala sobre pressão no Santos, avalia e...   \n",
       "..                                                 ...   \n",
       "118  Cruzeiro quer Gabigol, do Flamengo, após desis...   \n",
       "119  Lutando até o fim, Sérvia arranca empate da Es...   \n",
       "120  Veja o time ideal da 6ª rodada da Copa Sul-Ame...   \n",
       "121  Grêmio perde mais uma e iguala feito negativo ...   \n",
       "122  Bahia x Fortaleza: onde assistir ao vivo, horá...   \n",
       "\n",
       "                                              Título 2                Data  \\\n",
       "0    Jogo contra o Palmeiras foi marcado pela polêm... 2024-06-18 20:27:00   \n",
       "1    Empresário pediu união entre as equipes contra... 2024-06-18 15:11:00   \n",
       "2    Lateral-direito tem contrato com a Juventus at... 2024-06-19 11:09:00   \n",
       "3    Meio-campista joga bastante pelo Internacional... 2024-06-19 16:27:00   \n",
       "4    Após quatro derrotas consecutivas na Série B, ... 2024-06-19 22:38:00   \n",
       "..                                                 ...                 ...   \n",
       "118  Atacante rubro-negro seria alvo da Raposa por ... 2024-06-18 10:06:00   \n",
       "119  Seleção eslovena vencia até os 95 minutos e sé... 2024-06-20 12:07:00   \n",
       "120  Athletico conta com dois representantes na sel... 2024-06-01 07:00:00   \n",
       "121  Imortal foi superado pelo Fortaleza, na noite ... 2024-06-20 14:16:00   \n",
       "122  Confira todos os detalhes do duelo válido pela... 2024-06-13 07:46:00   \n",
       "\n",
       "                   Local                                           Conteúdo  \n",
       "0    Belo Horizonte (MG)  - Quando John Textor fez as denúncias que todo...  \n",
       "1    Belo Horizonte (MG)  - O Atlético tomará todas as medidas cabíveis,...  \n",
       "2    Rio de Janeiro (RJ)  – A minha família é toda flamenguista, é uma l...  \n",
       "3         Rio de Janeiro  Natural de São Paulo, Maurício iniciou sua car...  \n",
       "4         São Paulo (SP)  – Me sinto confortável aqui, mas sei que se eu...  \n",
       "..                   ...                                                ...  \n",
       "118  Rio de Janeiro (RJ)  Sócio-majoritário da Sociedade Anônima do Fute...  \n",
       "119       Rio de Janeiro  O goleiro da Eslovênia, Oblak teve um primeiro...  \n",
       "120       São Paulo (SP)  Apesar dos triunfos, as equipes não contam com...  \n",
       "121  Rio de Janeiro (RJ)  Após mais uma derrota, desta vez para o Fortal...  \n",
       "122        Salvador (BA)  Confira todas as informações que você precisa ...  \n",
       "\n",
       "[123 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticias[\"Data\"] = pd.to_datetime(noticias[\"Data\"])\n",
    "noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarando o nome do arquivo a salvar os conteúdos extraídos\n",
    "arquivo = \"noticias_lance.csv\"\n",
    "noticias.to_csv(arquivo,encoding=\"utf-8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
